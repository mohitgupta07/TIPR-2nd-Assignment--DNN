{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "np.random.seed(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X,Y):\n",
    "        #Cross-validation -- to be done via k-fold later.\n",
    "        from sklearn.model_selection import train_test_split  \n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20)\n",
    "        return X_train, Y_train,X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X,normalize=False,gaxis=1):\n",
    "    from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    if(normalize):\n",
    "        X= sklearn.preprocessing.normalize(X,axis=gaxis)\n",
    "    #print(X_S.shape)\n",
    "    X=scaler.fit_transform(X)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotGraph(net,costs,fig_name):\n",
    "    #plt.plot(t, t, 'r--', t, t**2, 'bs', t, t**3, 'g^')\n",
    "    aa=list(costs.values())\n",
    "    aa=np.array([list(i) for i in  aa])\n",
    "    a1,a2,a3=aa.T[0],aa.T[1],aa.T[2] #accuracy, cost\n",
    "    plt.figure(num=None, figsize=(8, 6), dpi=600, facecolor='w', edgecolor='k')\n",
    "    #write after this line.\n",
    "    plt.xlabel('no. of epochs')\n",
    "    plt.ylabel(\"Accuracy/Cost<Scaled-down by max={0}>\".format(int(np.max(a2))))\n",
    "    plt.title('Dataset={1}, Layers={3}, Costs={2},\\nActivators={0}\\nWeight-Init={4}'.\n",
    "              format(net.activations,net.dataSetName,net.costName,net.layers,'random/normal'))\n",
    "    plt.subplot().plot(list(costs.keys()),a1,'r--',label='Accuracy on Train Set')\n",
    "    plt.subplot().plot(list(costs.keys()),a2/np.max(a2),'b', label='Cost of Train Data')\n",
    "    plt.subplot().plot(list(costs.keys()),a3,'b--', label='Accuracy on Validation Set')\n",
    "    \n",
    "    plt.legend(loc='center right', shadow=True, fontsize='x-large')\n",
    "\n",
    "    plt.savefig(fig_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot(y,gClasses=None):\n",
    "        S=list(set(y))\n",
    "        if (gClasses):\n",
    "            S=list(gClasses.values())\n",
    "        classes={}\n",
    "        #Y=np.zeros( (len(y),len(classes)))\n",
    "        for i in range(len(S)):\n",
    "            classes[i]=S[i]\n",
    "        Y=[ [0 for i in range(len(S)) ] for _ in range(len(y))]\n",
    "        for i in range(len(y)):\n",
    "            #print(i,classes.index(y[i]))\n",
    "            Y[i][S.index(y[i])]+=1\n",
    "            #print(Y[i],classes.index(y[i]),i)\n",
    "            \n",
    "        return Y,classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetwork():\n",
    "    def __init__(self,X,y,classes=None,oneHot=True,dataSetName=\"\",weightInit=\"random\",hiddenlayers=[128,35],activations=['relu','tanh','soft-max'],cost='L2',learningRate=[0.1,0.01,0.001]):\n",
    "        self.dataSetName=dataSetName\n",
    "        self.weightInit=\"random\"\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        self.classes=classes\n",
    "        self.counter=0\n",
    "        self.y=np.array(self.y)\n",
    "        self.isOneHot=oneHot\n",
    "        \n",
    "        self.myactivators={'sigmoid':self.sigmoid,\n",
    "                      'tanh':self.tanh,\n",
    "                      'soft-max':self.softmax,'Identity':self.Identity,'relu':self.relu}\n",
    "        self.mycosts={'L2':self.L2_cost}\n",
    "        self.hiddenlayers=hiddenlayers\n",
    "        self.layers=hiddenlayers\n",
    "        self.layers.insert(0,self.X.shape[1])\n",
    "        self.layers.append(self.y.shape[1])\n",
    "        self.activations=activations\n",
    "        self.methods=[ self.myactivators[i] for i in activations]\n",
    "        self.learningRate=learningRate\n",
    "        self.costName=cost\n",
    "        self.cost=self.mycosts[cost]\n",
    "        self.createLayers()\n",
    "        self.initBias()\n",
    "    def fitOnOtherDataSet(self,X,y,oneHot=True):\n",
    "        self.X=self.scale(np.array(X))\n",
    "        self.y=self.oneHot(y)\n",
    "            \n",
    "    def dep_fit_train(X,y,self,batch_size=32,epochs=3,batch_iterations=10,doOneHot=True):\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        n=len(y)\n",
    "        if(doOneHot):\n",
    "            self.y,self.classes=self.oneHot(self.y)\n",
    "        for epoch in range(epochs):\n",
    "            print(\"epoch:{0}\".format(epoch+1))\n",
    "            inx=0\n",
    "            while(inx<n):\n",
    "                if(inx+batch_size>n):\n",
    "                    Y=self.y[inx:]\n",
    "                    X=self.X[inx:]\n",
    "                else:    \n",
    "                    Y=self.y[inx:inx+batch_size]\n",
    "                    X=self.X[inx:inx+batch_size]\n",
    "                \n",
    "                self.train(X,Y)\n",
    "                inx+=batch_size\n",
    "            y_pred=self.getPredictions(X)\n",
    "            print(\"Accuracy:\",self.getAccuracy(self.y,y_pred))\n",
    "    def dep_train(self,X,y,itr=1000):\n",
    "        for _ in range(itr):\n",
    "            if(_%100==0):\n",
    "                print(\"training model at {0}th iteration\".format(_))\n",
    "            A,Z=self.feedForward(X,self.methods)\n",
    "            self.backprop(A,Z,self.methods,y,self.cost,len(X))\n",
    "        \n",
    "    def getPredictions(self,X):\n",
    "        z=X\n",
    "        for i in range(len(self.layers)-1):\n",
    "            w=self.weights[i]\n",
    "            b=self.bias[i+1]#1xk\n",
    "            a=np.add( np.dot(z,w) , b) #mxn nxk= mxk  -- wx+b\n",
    "            z=self.methods[i](a)\n",
    "        #print(z.shape)\n",
    "        yp=np.argmax(z,axis=1)\n",
    "        #z=oneHot(yp,self.classes)\n",
    "        #print(z.shape)\n",
    "        #z[z>=yp]=1\n",
    "        #z[z<yp]=0\n",
    "        return yp\n",
    "    def getOriginalClassIndex(self,z):#getOriginalClassIndex\n",
    "        return np.argmax(z,axis=1)\n",
    "    def getAccuracy(self,y_1,y_2):#Classification !\n",
    "        return np.mean(y_1==y_2)\n",
    "        \n",
    "        \n",
    "    def createLayers(self,diminish=0.01,init_type='random'):\n",
    "        self.weights=[]\n",
    "        for i in range(len(self.layers)-1):\n",
    "            self.weights.append( np.random.rand(self.layers[i],self.layers[i+1]) *diminish)\n",
    "    \n",
    "    def initBias(self):\n",
    "        self.bias=[[]]\n",
    "        for i in range(1,len(self.layers)):\n",
    "            self.bias.append( np.zeros((1,self.layers[i])))\n",
    "        print('size of bias:',len(self.bias))\n",
    "    \n",
    "    def train(self,epochs=500,earlyStopping=False,X_val=None,y_val=None,minEpochs=100,patience=10): \n",
    "        #y_val== onehot vector.\n",
    "        acc_val,acc_main=0,1e100\n",
    "        isUP=False\n",
    "        Costs={}\n",
    "        yp_ind=self.getOriginalClassIndex(self.y)\n",
    "        if(earlyStopping):\n",
    "            X_val=np.array(X_val)\n",
    "            #print(y_val)\n",
    "            y_val=self.getOriginalClassIndex(np.array(y_val))\n",
    "            #print(y_val)\n",
    "        for _ in range(epochs):\n",
    "            self.counter+=1\n",
    "            if((self.counter)%10==0):print((self.counter),end=' ')\n",
    "            cost=0\n",
    "            for i in range(len(self.X)):\n",
    "                A,Z=self.feedForward(np.array(self.X[i:i+1]),self.methods)\n",
    "                cost+=self.backprop(A,Z,self.methods,np.array(self.y[i:i+1]),self.L2_cost,returnCost=True)\n",
    "            if(earlyStopping):\n",
    "                y_val_pred=self.getPredictions(X_val)\n",
    "                #print(y_val_pred)\n",
    "                tmp=self.getAccuracy(y_val,y_val_pred)\n",
    "                if(isUP and tmp<acc_val and self.counter>minEpochs):\n",
    "                    if(patience==10):\n",
    "                        self.saveModel('{0}_patience_at_{1}'.format(self.dataSetName,self.counter))\n",
    "                        #np.save('{0}_patience_0'.format(self.dataSetName))\n",
    "                    patience-=1\n",
    "                    if(patience==0):\n",
    "                        break\n",
    "                if(tmp>acc_val):\n",
    "                    acc_val=tmp\n",
    "                    isUP=True            \n",
    "                    \n",
    "            y_pred=self.getPredictions(self.X)\n",
    "            acc_main=self.getAccuracy(yp_ind,y_pred)\n",
    "            #acc_main=np.mean(y_pred==self.y_orig.T)\n",
    "            if((self.counter)%10==0):  \n",
    "                print(\"Cost:\",cost,\"acc:\",acc_main, 'validation_acc:',acc_val)\n",
    "            if(earlyStopping):\n",
    "                Costs[self.counter]=[acc_main,acc_val,cost]\n",
    "            else:\n",
    "                Costs[self.counter]=[acc_main,cost]\n",
    "        return Costs\n",
    "    \n",
    "    def saveModel(self,modelName):\n",
    "        np.save(modelName,self.weights)\n",
    "    \n",
    "    def feedForward(self,X,method):\n",
    "        '''\n",
    "        Note X-- nxd -- represents n= images with d dim.\n",
    "        W[0]=layer[0] X layer[1] or d x l1\n",
    "        so a[1]= np.dot(X, W[0])\n",
    "        z[1]=activator(a[1]) can be sigmoid/relu/tanh/squish etc...\n",
    "        '''\n",
    "        Z=[X]\n",
    "        A=[[]]\n",
    "        for i in range(len(self.layers)-1):\n",
    "            w=self.weights[i]#nxk\n",
    "            b=self.bias[i+1]#1xk\n",
    "            a=np.add( np.dot(Z[i],w) , b) #mxn nxk= mxk  -- wx+b\n",
    "            A.append(np.array(a))\n",
    "            z=method[i](a)\n",
    "            Z.append(np.array(z))\n",
    "            \n",
    "            #print(\"A Z shape\",A[-1].shape,Z[-1].shape)\n",
    "        return A,Z\n",
    "    \n",
    "    def backprop(self,A,Z,method,y,cost,printCost=False,returnCost=True):\n",
    "        #here it should be no. of samples--batch size\n",
    "        #print(\"z,y,shapes\",Z[-1].shape,y.shape)\n",
    "        E=cost(Z[-1],y)\n",
    "        if(printCost):\n",
    "            print(\"COST:\",E)\n",
    "        dEdOout=cost(Z[-1],y,derivative=True)# 1D-vector. Actually its m X 1d-vector\n",
    "        dOoutdOin=method[-1](A[-1],derivative=True)#1D-vector\n",
    "        dOindw=Z[-2]#HlastOut 1D-vector\n",
    "        #print(\"dOindw nx14\",dOindw.shape)\n",
    "        #####\n",
    "        dEdOin=np.multiply(dEdOout,dOoutdOin)#This is write  \n",
    "        #print('dEdOin shape',dEdOin.shape)\n",
    "        '''\n",
    "        n=1\n",
    "        dEdw=np.matmul(dOindw.reshape(-1,n),dEdOin.reshape(n,-1)) # (Hlast,n)* (n,Oin) -- can cause problem for batch-grad\n",
    "        '''\n",
    "        dEdw=np.matmul(dOindw.T,dEdOin) # (Hlast,n)* (n,Oin) -- can cause problem for batch-grad\n",
    "        #print('dedw shape',dEdw.shape)\n",
    "        self.weights[-1]-=self.learningRate[-1]*dEdw\n",
    "        self.bias[-1]-=self.learningRate[-1]*np.sum(dEdOin,axis=0)\n",
    "        #print('dedw:{0}\\ndEdOin:{1}\\ndEdOout:{2}\\ndOoutdOin:{3}'.format(dEdw,dEdOin,dEdOout,dOoutdOin))\n",
    "        #### Do general Recursion Now.\n",
    "        #Call dEdOin as delta\n",
    "        delta= dEdOin\n",
    "        #print('delta:',delta.shape)\n",
    "          \n",
    "        # Weights=[in * h1, h1 *h2, h2 * hlast, hlast * out]\n",
    "        # Already Calculated hlast* out or weights[-1]\n",
    "        for i in range(len(self.weights)-2,-1,-1):\n",
    "            '''\n",
    "            size(Z)=size(A)=size(weights)+1\n",
    "            '''\n",
    "            dHoutdHin=method[i](A[i+1],derivative=True)\n",
    "            dHindw=Z[i]\n",
    "            #dHindw=np.tile(dHindw.reshape(-1,1),self.weights[i].shape[1])\n",
    "            #print('dhindw',dHindw)\n",
    "            #Need to find dEtotaldHout=dEtotal_dOin*dOin_dHout\n",
    "            dEtotaldHout=np.matmul(delta,self.weights[i+1].T)\n",
    "            #print()\n",
    "            dEdHin=np.multiply(dEtotaldHout,dHoutdHin)     #refraining use of Etotal. jUst E now. \n",
    "            #print(\"e/hout\",dEtotaldHout,\"\\nhout/hin\",dHoutdHin,\"\\ne/hin\",dEdHin)\n",
    "            dEdw=np.matmul(dHindw.T,dEdHin) # (Hlast,1)* (1,Oin)\n",
    "            #print(dEdw.shape,dEdw)\n",
    "            self.weights[i]-=self.learningRate[i]*dEdw\n",
    "            #print(dEdHin.shape,np.sum(dEdHin,axis=0).shape,self.bias[i+1].shape)\n",
    "            self.bias[i+1]-=self.learningRate[-1]*np.sum(dEdHin,axis=0)\n",
    "            delta=dEdHin\n",
    "            #print('delta:',delta)\n",
    "        return E\n",
    "            \n",
    "        '''\n",
    "        np.repeat(z,3,axis=0).reshape(3,3)-x\n",
    "        try to make it for mini-batch over stochastic\n",
    "        '''\n",
    "        \n",
    "    def softmax(self,a,derivative=False):\n",
    "        z=np.exp(a)\n",
    "        if(derivative):\n",
    "            su=np.sum(z,axis=1).reshape(-1,1)#try to use np.sum(s,axis=1 for row-wise sum ; 0 for col-wise sum)\n",
    "            t=su-z\n",
    "            tsq=np.sum(z,axis=1).reshape(-1,1)**2\n",
    "            z=np.multiply(t,z)\n",
    "            return z/tsq\n",
    "        return z/np.sum(z)\n",
    "    \n",
    "    def relu(self,a,derivative=False):\n",
    "        if(derivative==True):\n",
    "            a[a<0]=0\n",
    "            a[a>0]=1\n",
    "            return a\n",
    "        a[a<0]=0\n",
    "        return a\n",
    "    def sigmoid(self,a,derivative=False):\n",
    "        \n",
    "        z= 1/(1+ np.exp(-a))\n",
    "        if(derivative ==True):\n",
    "            return np.multiply(z,(1-z))\n",
    "        return z\n",
    "    def tanh(self,a,derivative=False):\n",
    "        #z=(2/(1+np.exp(-2*a))) -1\n",
    "        if(derivative):\n",
    "             return (1 - (np.tanh(a) ** 2)) \n",
    "        return np.tanh(a)\n",
    "    \n",
    "    def Identity(self,a,derivative=False):\n",
    "        return a\n",
    "    \n",
    "    def L2_cost(self,A,B,derivative=False):\n",
    "        A=np.array(A)#OUT\n",
    "        B=np.array(B)#Actual output y\n",
    "        #print('cost:',A.shape, B.shape)\n",
    "        C=A-B\n",
    "        if(derivative):\n",
    "            return C\n",
    "        return np.sum(C**2)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X,y):\n",
    "        X=scale(X)\n",
    "        y,classes=oneHot(y)\n",
    "        return X,y,classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "X,y=sklearn.datasets.load_digits(n_class=10, return_X_y=True)\n",
    "X,y,classes=preprocess(X,y)\n",
    "X_train,y_train,X_val,y_val=train_test_split(X,y)\n",
    "X_train=np.array(X_train)\n",
    "y_train=np.array(y_train)\n",
    "X_val=np.array(X_val)\n",
    "y_val=np.array(y_val)\n",
    "\n",
    "#X,y=sklearn.datasets.load_iris(return_X_y=True)\n",
    "gm1=X.shape[1]*2\n",
    "gm2=int((gm1*10)**(0.5))\n",
    "#X_train=np.copy(X)\n",
    "#y_train=np.copy(y)\n",
    "#classes=[i for i in range(0,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of bias: 4\n"
     ]
    }
   ],
   "source": [
    "#net=neuralNetwork(X,y.reshape(-1,1),hiddenlayers=[gm1],activations=['tanh','soft-max'],cost='L2',learningRate=[.1,.001])#iris\n",
    "\n",
    "net=neuralNetwork(X_train,y_train,classes,dataSetName=\"MNIST\",hiddenlayers=[gm1,gm2],activations=['tanh','tanh','soft-max'],cost='L2',learningRate=[.1,.01,.001])#mnist\n",
    "#net.y,net.classes=net.oneHot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far For Mnist Dataset.\n",
    "Config is:-<br>\n",
    "<b>net=neuralNetwork(X,y.reshape(-1,1),hiddenlayers=[128,gm],activations=['relu','tanh','soft-max'],cost='L2',learningRate=[.01,.01,.01])</b>  acc = 99.99% <br><br>\n",
    "net=neuralNetwork(X,y.reshape(-1,1),hiddenlayers=[128,gm],activations=['relu','relu','soft-max'],cost='L2',learningRate=[.01,.01,.01]) acc= 99% but chokes for NAN<br>\n",
    "<br>For Scale :-\n",
    "Normalize is set to false for both case. Min-max works better. with norm then minmax, cost goes down but slower than just minmax.\n",
    "<br>Seed set to 26.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[64, 128, 35, 10]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "net.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_val,_cl=net.oneHot(y_val_orig)\n",
    "net.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Cost: 1293.265758209887 acc: 0.10368823938761308 validation_acc: 0.09444444444444444\n",
      "20 Cost: 1293.1313319345948 acc: 0.10368823938761308 validation_acc: 0.09444444444444444\n",
      "30 Cost: 1292.0384119747505 acc: 0.10368823938761308 validation_acc: 0.09444444444444444\n",
      "40 Cost: 1230.5444117832105 acc: 0.19693806541405706 validation_acc: 0.225\n",
      "50 Cost: 1140.3312775070192 acc: 0.2045929018789144 validation_acc: 0.225\n",
      "60 Cost: 1122.3621881193308 acc: 0.21085594989561587 validation_acc: 0.225\n",
      "70 Cost: 1081.7866378506365 acc: 0.28114126652748783 validation_acc: 0.2861111111111111\n",
      "80 Cost: 999.1347829416156 acc: 0.43354210160055673 validation_acc: 0.45\n",
      "90 Cost: 913.1526647647702 acc: 0.5156576200417536 validation_acc: 0.49444444444444446\n",
      "100 Cost: 777.8458824207372 acc: 0.5915100904662491 validation_acc: 0.5694444444444444\n",
      "110 Cost: 624.5907902847744 acc: 0.7265135699373695 validation_acc: 0.7027777777777777\n",
      "120 Cost: 373.13007113174274 acc: 0.9095337508698678 validation_acc: 0.8583333333333333\n",
      "130 Cost: 180.00335326493078 acc: 0.9665970772442589 validation_acc: 0.9055555555555556\n",
      "140 Cost: 113.06071113691576 acc: 0.97633959638135 validation_acc: 0.925\n",
      "150 Cost: 80.72800678942762 acc: 0.9832985386221295 validation_acc: 0.9277777777777778\n",
      "160 Cost: 61.85969480172545 acc: 0.9867780097425192 validation_acc: 0.9277777777777778\n",
      "170 Cost: 50.16924657647947 acc: 0.988169798190675 validation_acc: 0.9305555555555556\n",
      "180 "
     ]
    }
   ],
   "source": [
    "#costs=net.train(epochs=1000)\n",
    "costs=net.train(epochs=1000,earlyStopping=True,X_val=X_val,y_val=y_val)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yp=net.getPredictions(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y==yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_name='myPlot2.png'\n",
    "plotGraph(net,costs,fig_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(net.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.argmax(y_val,axis=1)==yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
